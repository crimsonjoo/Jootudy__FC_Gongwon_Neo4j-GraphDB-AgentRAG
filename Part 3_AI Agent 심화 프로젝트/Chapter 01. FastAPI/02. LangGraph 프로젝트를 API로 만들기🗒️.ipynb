{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed41a75",
   "metadata": {},
   "source": [
    "**Part 3. AI Agent 심화 프로젝트**\n",
    "\n",
    "- Chapter 01. FastAPI\n",
    "\n",
    "    - **📒[실습]** Clip 02. LangGraph 프로젝트를 API로 만들기🗒️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efb2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response : LangGraph의 Memory 문서는 에이전트를 구축하는 데 필수적인 단기 기억과 장기 기억을 다루며, 단기 기억은 세션 내에서 메시지 기록을 유지하여 대화를 추적하고, 장기 기억은 사용자의 특정 데이터나 애플리케이션 수준의 데이터를 세션 간에 저장하는 방식을 설명합니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/ai-assist/invoke\"\n",
    "# payload = {\"message\": \"안녕\"}\n",
    "payload = {\"message\": \"https://langchain-ai.github.io/langgraph/agents/memory/ 이 내용을 한문장으로 요약해주세요.\"}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response :\", response.json()['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a11be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Streaming response:\n",
      "[Node update] {\"chatbot\": \"\"}\n",
      "[Node update] {\"web_scraper\": \"<Document name=\\\"Memory\\\">\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMemory\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\nWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Memory\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n    Quickstart\\n    \\n  \\n\\n\\n\\n\\n\\n    LangGraph basics\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Prebuilt agents\\n    \\n  \\n\\n\\n\\n\\n\\n            Prebuilt agents\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Running agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n    MCP Integration\\n    \\n  \\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Short-term memory\\n    \\n\\n\\n\\n\\n\\n\\n      Manage message history\\n    \\n\\n\\n\\n\\n\\n\\n      Summarize message history\\n    \\n\\n\\n\\n\\n\\n      Trim message history\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Read in tools\\n    \\n\\n\\n\\n\\n\\n      Write from tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Long-term memory\\n    \\n\\n\\n\\n\\n\\n\\n      Read\\n    \\n\\n\\n\\n\\n\\n      Write\\n    \\n\\n\\n\\n\\n\\n      Semantic search\\n    \\n\\n\\n\\n\\n\\n      Prebuilt memory tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Additional resources\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Evals\\n    \\n  \\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n    UI\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph framework\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph framework\\n          \\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Graphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Breakpoints\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph Platform\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Components\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Threads\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    Breakpoints\\n    \\n  \\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Short-term memory\\n    \\n\\n\\n\\n\\n\\n\\n      Manage message history\\n    \\n\\n\\n\\n\\n\\n\\n      Summarize message history\\n    \\n\\n\\n\\n\\n\\n      Trim message history\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Read in tools\\n    \\n\\n\\n\\n\\n\\n      Write from tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Long-term memory\\n    \\n\\n\\n\\n\\n\\n\\n      Read\\n    \\n\\n\\n\\n\\n\\n      Write\\n    \\n\\n\\n\\n\\n\\n      Semantic search\\n    \\n\\n\\n\\n\\n\\n      Prebuilt memory tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Additional resources\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nagent\\n\\n\\n\\n\\nMemory¶\\nLangGraph supports two types of memory essential for building conversational agents:\\n\\nShort-term memory: Tracks the ongoing conversation by maintaining message history within a session.\\nLong-term memory: Stores user-specific or application-level data across sessions.\\n\\nThis guide demonstrates how to use both memory types with agents in LangGraph. For a deeper\\nunderstanding of memory concepts, refer to the LangGraph memory documentation.\\n\\n\\nBoth short-term and long-term memory require persistent storage to maintain continuity across LLM interactions. In production environments, this data is typically stored in a database.\\n\\n\\nTerminology\\nIn LangGraph:\\n\\nShort-term memory is also referred to as thread-level memory.\\nLong-term memory is also called cross-thread memory.\\n\\nA thread represents a sequence of related runs\\ngrouped by the same thread_id.\\n\\nShort-term memory¶\\nShort-term memory enables agents to track multi-turn conversations. To use it, you must:\\n\\nProvide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state.\\nSupply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session.\\n\\nAPI Reference: create_react_agent | InMemorySaver\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\ncheckpointer = InMemorySaver() # (1)!\\n\\n\\ndef get_weather(city: str) -> str:\\n    \\\"\\\"\\\"Get weather for a given city.\\\"\\\"\\\"\\n    return f\\\"It's always sunny in {city}!\\\"\\n\\n\\nagent = create_react_agent(\\n    model=\\\"anthropic:claude-3-7-sonnet-latest\\\",\\n    tools=[get_weather],\\n    checkpointer=checkpointer # (2)!\\n)\\n\\n# Run the agent\\nconfig = {\\n    \\\"configurable\\\": {\\n        \\\"thread_id\\\": \\\"1\\\"  # (3)!\\n    }\\n}\\n\\nsf_response = agent.invoke(\\n    {\\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"what is the weather in sf\\\"}]},\\n    config\\n)\\n\\n# Continue the conversation using the same thread_id\\nny_response = agent.invoke(\\n    {\\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"what about new york?\\\"}]},\\n    config # (4)!\\n)\\n\\n\\nThe InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you.\\nThe checkpointer is passed to the agent. This enables the agent to persist its state across invocations.\\nA unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string.\\nThe agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. \\n\\nWhen the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York.\\n\\nLangGraph Platform providers a production-ready checkpointer\\nIf you're using LangGraph Platform, during deployment your checkpointer will be automatically configured to use a production-ready database.\\n\\nManage message history¶\\nLong conversations can exceed the LLM's context window. Common solutions are:\\n\\nSummarization: Maintain a running summary of the conversation\\nTrimming: Remove first or last N messages in the history\\n\\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\\nTo manage message history, specify pre_model_hook — a function (node) that will always run before calling the language model.\\nSummarize message history¶\\n\\n\\nLong conversations can exceed the LLM's context window. A common solution is to maintain a running summary of the conversation. This allows the agent to keep track of the conversation without exceeding the LLM's context window.\\n\\n\\nTo summarize message history, you can use pre_model_hook with a prebuilt SummarizationNode:\\nAPI Reference: ChatAnthropic | count_tokens_approximately | create_react_agent | AgentState | InMemorySaver\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langmem.short_term import SummarizationNode\\nfrom langchain_core.messages.utils import count_tokens_approximately\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom typing import Any\\n\\nmodel = ChatAnthropic(model=\\\"claude-3-7-sonnet-latest\\\")\\n\\nsummarization_node = SummarizationNode( # (1)!\\n    token_counter=count_tokens_approximately,\\n    model=model,\\n    max_tokens=384,\\n    max_summary_tokens=128,\\n    output_messages_key=\\\"llm_input_messages\\\",\\n)\\n\\nclass State(AgentState):\\n    # NOTE: we're adding this key to keep track of previous summary information\\n    # to make sure we're not summarizing on every LLM call\\n    context: dict[str, Any]  # (2)!\\n\\n\\ncheckpointer = InMemorySaver() # (3)!\\n\\nagent = create_react_agent(\\n    model=model,\\n    tools=tools,\\n    pre_model_hook=summarization_node, # (4)!\\n    state_schema=State, # (5)!\\n    checkpointer=checkpointer,\\n)\\n\\n\\nThe InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you.\\nThe context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient.\\nThe checkpointer is passed to the agent. This enables the agent to persist its state across invocations.\\nThe pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the create_react_agent API reference for more details.\\nThe state_schema is set to the State class, which is the custom state that contains an extra context key.\\n\\nTrim message history¶\\nTo trim message history, you can use pre_model_hook with trim_messages function:\\nAPI Reference: trim_messages | count_tokens_approximately | create_react_agent\\nfrom langchain_core.messages.utils import (\\n    trim_messages,\\n    count_tokens_approximately\\n)\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# This function will be called every time before the node that calls LLM\\ndef pre_model_hook(state):\\n    trimmed_messages = trim_messages(\\n        state[\\\"messages\\\"],\\n        strategy=\\\"last\\\",\\n        token_counter=count_tokens_approximately,\\n        max_tokens=384,\\n        start_on=\\\"human\\\",\\n        end_on=(\\\"human\\\", \\\"tool\\\"),\\n    )\\n    return {\\\"llm_input_messages\\\": trimmed_messages}\\n\\ncheckpointer = InMemorySaver()\\nagent = create_react_agent(\\n    model,\\n    tools,\\n    pre_model_hook=pre_model_hook,\\n    checkpointer=checkpointer,\\n)\\n\\nTo learn more about using pre_model_hook for managing message history, see this how-to guide\\nRead in tools¶\\nLangGraph allows agent to access its short-term memory (state) inside the tools.\\nAPI Reference: InjectedState | create_react_agent\\nfrom typing import Annotated\\nfrom langgraph.prebuilt import InjectedState, create_react_agent\\n\\nclass CustomState(AgentState):\\n    user_id: str\\n\\ndef get_user_info(\\n    state: Annotated[CustomState, InjectedState]\\n) -> str:\\n    \\\"\\\"\\\"Look up user info.\\\"\\\"\\\"\\n    user_id = state[\\\"user_id\\\"]\\n    return \\\"User is John Smith\\\" if user_id == \\\"user_123\\\" else \\\"Unknown user\\\"\\n\\nagent = create_react_agent(\\n    model=\\\"anthropic:claude-3-7-sonnet-latest\\\",\\n    tools=[get_user_info],\\n    state_schema=CustomState,\\n)\\n\\nagent.invoke({\\n    \\\"messages\\\": \\\"look up user information\\\",\\n    \\\"user_id\\\": \\\"user_123\\\"\\n})\\n\\nSee the Context guide for more information.\\nWrite from tools¶\\nTo modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\\nAPI Reference: InjectedToolCallId | RunnableConfig | ToolMessage | InjectedState | create_react_agent | AgentState | Command\\nfrom typing import Annotated\\nfrom langchain_core.tools import InjectedToolCallId\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain_core.messages import ToolMessage\\nfrom langgraph.prebuilt import InjectedState, create_react_agent\\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\\nfrom langgraph.types import Command\\n\\nclass CustomState(AgentState):\\n    user_name: str\\n\\ndef update_user_info(\\n    tool_call_id: Annotated[str, InjectedToolCallId],\\n    config: RunnableConfig\\n) -> Command:\\n    \\\"\\\"\\\"Look up and update user info.\\\"\\\"\\\"\\n    user_id = config[\\\"configurable\\\"].get(\\\"user_id\\\")\\n    name = \\\"John Smith\\\" if user_id == \\\"user_123\\\" else \\\"Unknown user\\\"\\n    return Command(update={\\n        \\\"user_name\\\": name,\\n        # update the message history\\n        \\\"messages\\\": [\\n            ToolMessage(\\n                \\\"Successfully looked up user information\\\",\\n                tool_call_id=tool_call_id\\n            )\\n        ]\\n    })\\n\\ndef greet(\\n    state: Annotated[CustomState, InjectedState]\\n) -> str:\\n    \\\"\\\"\\\"Use this to greet the user once you found their info.\\\"\\\"\\\"\\n    user_name = state[\\\"user_name\\\"]\\n    return f\\\"Hello {user_name}!\\\"\\n\\nagent = create_react_agent(\\n    model=\\\"anthropic:claude-3-7-sonnet-latest\\\",\\n    tools=[update_user_info, greet],\\n    state_schema=CustomState\\n)\\n\\nagent.invoke(\\n    {\\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"greet the user\\\"}]},\\n    config={\\\"configurable\\\": {\\\"user_id\\\": \\\"user_123\\\"}}\\n)\\n\\nFor more details, see how to update state from tools.\\nLong-term memory¶\\nUse long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.\\nTo use long-term memory, you need to:\\n\\nConfigure a store to persist data across invocations.\\nUse the get_store function to access the store from within tools or prompts.\\n\\nRead¶\\nA tool the agent can use to look up user informationfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.config import get_store\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.store.memory import InMemoryStore\\n\\nstore = InMemoryStore() # (1)!\\n\\nstore.put(  # (2)!\\n    (\\\"users\\\",),  # (3)!\\n    \\\"user_123\\\",  # (4)!\\n    {\\n        \\\"name\\\": \\\"John Smith\\\",\\n        \\\"language\\\": \\\"English\\\",\\n    } # (5)!\\n)\\n\\ndef get_user_info(config: RunnableConfig) -> str:\\n    \\\"\\\"\\\"Look up user info.\\\"\\\"\\\"\\n    # Same as that provided to `create_react_agent`\\n    store = get_store() # (6)!\\n    user_id = config[\\\"configurable\\\"].get(\\\"user_id\\\")\\n    user_info = store.get((\\\"users\\\",), user_id) # (7)!\\n    return str(user_info.value) if user_info else \\\"Unknown user\\\"\\n\\nagent = create_react_agent(\\n    model=\\\"anthropic:claude-3-7-sonnet-latest\\\",\\n    tools=[get_user_info],\\n    store=store # (8)!\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"look up user information\\\"}]},\\n    config={\\\"configurable\\\": {\\\"user_id\\\": \\\"user_123\\\"}}\\n)\\n\\n\\nThe InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.\\nFor this example, we write some sample data to the store using the put method. Please see the BaseStore.put API reference for more details.\\nThe first argument is the namespace. This is used to group related data together. In this case, we are using the users namespace to group user data.\\nA key within the namespace. This example uses a user ID for the key.\\nThe data that we want to store for the given user.\\nThe get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.\\nThe get method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a StoreValue object, which contains the value and metadata about the value.\\nThe store is passed to the agent. This enables the agent to access the store when running tools. You can also use the get_store function to access the store from anywhere in your code.\\n\\nWrite¶\\nExample of a tool that updates user informationfrom typing_extensions import TypedDict\\n\\nfrom langgraph.config import get_store\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.store.memory import InMemoryStore\\n\\nstore = InMemoryStore() # (1)!\\n\\nclass UserInfo(TypedDict): # (2)!\\n    name: str\\n\\ndef save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)!\\n    \\\"\\\"\\\"Save user info.\\\"\\\"\\\"\\n    # Same as that provided to `create_react_agent`\\n    store = get_store() # (4)!\\n    user_id = config[\\\"configurable\\\"].get(\\\"user_id\\\")\\n    store.put((\\\"users\\\",), user_id, user_info) # (5)!\\n    return \\\"Successfully saved user info.\\\"\\n\\nagent = create_react_agent(\\n    model=\\\"anthropic:claude-3-7-sonnet-latest\\\",\\n    tools=[save_user_info],\\n    store=store\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"My name is John Smith\\\"}]},\\n    config={\\\"configurable\\\": {\\\"user_id\\\": \\\"user_123\\\"}} # (6)!\\n)\\n\\n# You can access the store directly to get the value\\nstore.get((\\\"users\\\",), \\\"user_123\\\").value\\n\\n\\nThe InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.\\nThe UserInfo class is a TypedDict that defines the structure of the user information. The LLM will use this to format the response according to the schema.\\nThe save_user_info function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.\\nThe get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.\\nThe put method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.\\nThe user_id is passed in the config. This is used to identify the user whose information is being updated.\\n\\nSemantic search¶\\nLangGraph also allows you to search for items in long-term memory by semantic similarity.\\nPrebuilt memory tools¶\\nLangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.\\nAdditional resources¶\\n\\nMemory in LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Context\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Human-in-the-loop\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie consent\\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. Clicking \\\"Accept\\\" makes our documentation better. Thank you! ❤️\\n\\n\\n\\n\\n\\n\\n\\n      GitHub\\n    \\n\\n\\n\\n\\nAccept\\nReject\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</Document>\"}\n",
      "[Node update] {\"chatbot\": \"LangGraph는 대화 에이전트 구축에 필수적인 단기 및 장기 메모리 기능을 지원하며, 단기 메모리는 세션 내 대화 기록을 유지하고, 장기 메모리는 사용자 또는 애플리케이션 수준 데이터를 세션 간에 저장합니다.\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/ai-assist/stream\"\n",
    "params = {\n",
    "    \"message\": \"https://langchain-ai.github.io/langgraph/agents/memory/ 한줄 요약 한글로!\"\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "with requests.get(url, params=params, headers=headers, stream=True) as response:\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"Streaming response:\")\n",
    "\n",
    "    for line in response.iter_lines(decode_unicode=True):\n",
    "        if line:  # 빈 줄이 아닐 경우 출력\n",
    "            print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b8755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c439083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
